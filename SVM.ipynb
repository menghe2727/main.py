{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#generalization error (also known as the out-of-sample error or the risk) is a\n",
    "# measure of how accurately an algorithm is able to predict outcome values for\n",
    "# previously unseen data\n",
    "\n",
    "from sklearn import svm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#C parameter a hyperplane with the largest minimum margin, and a hyperplane that\n",
    "# correctly separates as many instances as possible.\n",
    "# for large C, increase the correctly separates\n",
    "# for small C, increase the largest minimum margin\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# RBF kernel (Radial Basis Function)\n",
    "# k(x,x')=exp(-gamma||x-x'||^2)\n",
    "#gamma is scalar that defines how much influence a single training example has\n",
    "#For a low gamma, the model will be too constrained and include all points of the training dataset,\n",
    "# without really capturing the shape.\n",
    "#For a higher gamma, the model will capture the shape of the dataset well.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The cost parameter decides how much an SVM should be allowed to “bend” with the data. For a low cost,\n",
    "# you aim for a smooth decision surface and for a higher cost, you aim to classify more points correctly.\n",
    "# It is also simply referred to as the cost of misclassification.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#AIC Akaike Information Criterion (AIC). Derived from frequentist probability.\n",
    "#The AIC statistic is defined for logistic regression as follows\n",
    "#AIC = -2/N * LL + 2 * k/N\n",
    "#Where N is the number of examples in the training dataset, LL is the log-likelihood of the model on the training dataset,\n",
    "# and k is the number of parameters in the model.\n",
    "#The score, as defined above, is minimized, e.g. the model with the lowest AIC is selected.\n",
    "#Compared to the BIC method (below), the AIC statistic penalizes complex models less, meaning that it may put more emphasis\n",
    "# on model performance on the training dataset, and, in turn, select more complex models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BIC Bayesian Information Criterion (BIC). Derived from Bayesian probability.\n",
    "#BIC = -2 * LL + log(N) * k\n",
    "#Where log() has the base-e called the natural logarithm, LL is the log-likelihood of the model, N is the number of\n",
    "# examples in the training dataset, and k is the number of parameters in the model.\n",
    "# The score as defined above is minimized, e.g. the model with the lowest BIC is selected.\n",
    "#The quantity calculated is different from AIC, although can be shown to be proportional to the AIC. Unlike the AIC,\n",
    "# the BIC penalizes the model more for its complexity, meaning that more complex models will have a worse (larger) score and will, in turn, be less likely to be selected\n",
    "\n",
    "\n",
    "\n",
    "#MDL Minimum Description Length (MDL). Derived from information theory.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##variance and bias\n",
    "#The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to\n",
    "# miss the relevant relations between features and target outputs (underfitting).\n",
    "#The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an\n",
    "# algorithm modeling the random noise in the training data (overfitting)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}