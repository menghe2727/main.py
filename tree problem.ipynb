{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-tree-based-models/\n",
    "Skill test Questions and Answers\n",
    "1) Which of the following is/are true about bagging trees?\n",
    "\n",
    "In bagging trees, individual trees are independent of each other\n",
    "Bagging is the method for improving the performance by aggregating the results of weak learners\n",
    "A) 1\n",
    "B) 2\n",
    "C) 1 and 2\n",
    "D) None of these\n",
    "\n",
    "Solution: C\n",
    "\n",
    "Both options are true. In Bagging, each individual trees are independent of each other because they consider different subset of features and samples.\n",
    "\n",
    "\n",
    "\n",
    "2) Which of the following is/are true about boosting trees?\n",
    "\n",
    "In boosting trees, individual weak learners are independent of each other\n",
    "It is the method for improving the performance by aggregating the results of weak learners\n",
    "A) 1\n",
    "B) 2\n",
    "C) 1 and 2\n",
    "D) None of these\n",
    "\n",
    "Solution: B\n",
    "\n",
    "In boosting tree individual weak learners are not independent of each other because each tree correct the results of previous tree. Bagging and boosting both can be consider as improving the base learners results.\n",
    "\n",
    "\n",
    "\n",
    "3) Which of the following is/are true about Random Forest and Gradient Boosting ensemble methods?\n",
    "\n",
    "Both methods can be used for classification task\n",
    "Random Forest is use for classification whereas Gradient Boosting is use for regression task\n",
    "Random Forest is use for regression whereas Gradient Boosting is use for Classification task\n",
    "Both methods can be used for regression task\n",
    "A) 1\n",
    "B) 2\n",
    "C) 3\n",
    "D) 4\n",
    "E) 1 and 4\n",
    "\n",
    "Solution: E\n",
    "\n",
    "Both algorithms are design for classification as well as regression task.\n",
    "\n",
    "\n",
    "\n",
    "4) In Random forest you can generate hundreds of trees (say T1, T2 …..Tn) and then aggregate the results of these tree. Which of the following is true about individual(Tk) tree in Random Forest?\n",
    "\n",
    "Individual tree is built on a subset of the features\n",
    "Individual tree is built on all the features\n",
    "Individual tree is built on a subset of observations\n",
    "Individual tree is built on full set of observations\n",
    "A) 1 and 3\n",
    "B) 1 and 4\n",
    "C) 2 and 3\n",
    "D) 2 and 4\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Random forest is based on bagging concept, that consider faction of sample and faction of feature for building the individual trees.\n",
    "\n",
    "\n",
    "\n",
    "5) Which of the following is true about “max_depth” hyperparameter in Gradient Boosting?\n",
    "\n",
    "Lower is better parameter in case of same validation accuracy\n",
    "Higher is better parameter in case of same validation accuracy\n",
    "Increase the value of max_depth may overfit the data\n",
    "Increase the value of max_depth may underfit the data\n",
    "A) 1 and 3\n",
    "B) 1 and 4\n",
    "C) 2 and 3\n",
    "D) 2 and 4\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Increase the depth from the certain value of depth may overfit the data and for 2 depth values validation accuracies are same we always prefer the small depth in final model building.\n",
    "\n",
    "\n",
    "\n",
    "6) Which of the following algorithm doesn’t uses learning Rate as of one of its hyperparameter?\n",
    "\n",
    "Gradient Boosting\n",
    "Extra Trees\n",
    "AdaBoost\n",
    "Random Forest\n",
    "A) 1 and 3\n",
    "B) 1 and 4\n",
    "C) 2 and 3\n",
    "D) 2 and 4\n",
    "\n",
    "Solution: D\n",
    "\n",
    "Random Forest and Extra Trees don’t have learning rate as a hyperparameter.\n",
    "\n",
    "\n",
    "\n",
    "7) Which of the following algorithm would you take into the consideration in your final model building on the basis of performance?\n",
    "\n",
    "Suppose you have given the following graph which shows the ROC curve for two different classification algorithms such as Random Forest(Red) and Logistic Regression(Blue)\n",
    "\n",
    "\n",
    "\n",
    "A) Random Forest\n",
    "B) Logistic Regression\n",
    "C) Both of the above\n",
    "D) None of these\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Since, Random forest has largest AUC given in the picture so I would prefer Random Forest\n",
    "\n",
    "\n",
    "\n",
    "8) Which of the following is true about training and testing error in such case?\n",
    "\n",
    "Suppose you want to apply AdaBoost algorithm on Data D which has T observations. You set half the data for training and half for testing initially. Now you want to increase the number of data points for training T1, T2 … Tn where T1 < T2…. Tn-1 < Tn.\n",
    "\n",
    "A) The difference between training error and test error increases as number of observations increases\n",
    "B) The difference between training error and test error decreases as number of observations increases\n",
    "C) The difference between training error and test error will not change\n",
    "D) None of These\n",
    "\n",
    "Solution: B\n",
    "\n",
    "As we have more and more data, training error increases and testing error de-creases. And they all converge to the true error.\n",
    "\n",
    "\n",
    "\n",
    "9) In random forest or gradient boosting algorithms, features can be of any type. For example, it can be a continuous feature or a categorical feature. Which of the following option is true when you consider these types of features?\n",
    "\n",
    "A) Only Random forest algorithm handles real valued attributes by discretizing them\n",
    "B) Only Gradient boosting algorithm handles real valued attributes by discretizing them\n",
    "C) Both algorithms can handle real valued attributes by discretizing them\n",
    "D) None of these\n",
    "\n",
    "Solution: C\n",
    "\n",
    "Both can handle real valued features.\n",
    "\n",
    "\n",
    "\n",
    "10) Which of the following algorithm are not an example of ensemble learning algorithm?\n",
    "\n",
    "A) Random Forest\n",
    "B) Adaboost\n",
    "C) Extra Trees\n",
    "D) Gradient Boosting\n",
    "E) Decision Trees\n",
    "\n",
    "Solution: E\n",
    "\n",
    "Decision trees doesn’t aggregate the results of multiple trees so it is not an ensemble algorithm.\n",
    "\n",
    "\n",
    "\n",
    "11) Suppose you are using a bagging based algorithm say a RandomForest in model building. Which of the following can be true?\n",
    "\n",
    "Number of tree should be as large as possible\n",
    "You will have interpretability after using RandomForest\n",
    "A) 1\n",
    "B) 2\n",
    "C) 1 and 2\n",
    "D) None of these\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Since Random Forest aggregate the result of different weak learners, If It is possible we would want more number of trees in model building.  Random Forest is a black box model you will lose interpretability after using it.\n",
    "\n",
    "\n",
    "\n",
    "Context 12-15\n",
    "\n",
    "Consider the following figure for answering the next few questions. In the figure, X1 and X2 are the two features and the data point is represented by dots (-1 is negative class and +1 is a positive class). And you first split the data based on feature X1(say splitting point is x11) which is shown in the figure using vertical line. Every value less than x11 will be predicted as positive class and greater than x will be predicted as negative class.\n",
    "\n",
    "12) How many data points are misclassified in above image?\n",
    "\n",
    "A) 1\n",
    "B) 2\n",
    "C) 3\n",
    "D) 4\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Only one observation is misclassified, one negative class is showing at the left side of vertical line which will be predicting as a positive class.\n",
    "\n",
    "\n",
    "\n",
    "13) Which of the following splitting point on feature x1 will classify the data correctly?\n",
    "\n",
    "A) Greater than x11\n",
    "B) Less than x11\n",
    "C) Equal to x11\n",
    "D) None of above\n",
    "\n",
    "Solution: D\n",
    "\n",
    "If you search any point on X1 you won’t find any point that gives 100% accuracy.\n",
    "\n",
    "\n",
    "\n",
    "14) If you consider only feature X2 for splitting. Can you now perfectly separate the positive class from negative class for any one split on X2?\n",
    "\n",
    "A) Yes\n",
    "B) No\n",
    "\n",
    "Solution: B\n",
    "\n",
    "It is also not possible.\n",
    "\n",
    "\n",
    "\n",
    "15) Now consider only one splitting on both (one on X1 and one on X2) feature. You can split both features at any point. Would you be able to classify all data points correctly?\n",
    "\n",
    "A) TRUE\n",
    "B) FALSE\n",
    "\n",
    "Solution: B\n",
    "\n",
    "You won’t find such case because you can get minimum 1 misclassification.\n",
    "\n",
    "\n",
    "\n",
    "Context 16-17\n",
    "\n",
    "Suppose, you are working on a binary classification problem with 3 input features. And you chose to apply a bagging algorithm(X) on this data. You chose max_features = 2 and the n_estimators =3. Now, Think that each estimators have 70% accuracy.\n",
    "\n",
    "Note: Algorithm X is aggregating the results of individual estimators based on maximum voting\n",
    "\n",
    "16) What will be the maximum accuracy you can get?\n",
    "\n",
    "A) 70%\n",
    "B) 80%\n",
    "C) 90%\n",
    "D) 100%\n",
    "\n",
    "Solution: D\n",
    "\n",
    "Refer below table for models M1, M2 and M3.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Actual predictions\tM1\tM2\tM3\t Output\n",
    "1\t1\t0\t1\t1\n",
    "1\t1\t0\t1\t1\n",
    "1\t1\t0\t1\t1\n",
    "1\t0\t1\t1\t1\n",
    "1\t0\t1\t1\t1\n",
    "1\t0\t1\t1\t1\n",
    "1\t1\t1\t1\t1\n",
    "1\t1\t1\t0\t1\n",
    "1\t1\t1\t0\t1\n",
    "1\t1\t1\t0\t1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "17) What will be the minimum accuracy you can get?\n",
    "\n",
    "A) Always greater than 70%\n",
    "B) Always greater than and equal to 70%\n",
    "C) It can be less than 70%\n",
    "D) None of these\n",
    "\n",
    "Solution: C\n",
    "\n",
    "Refer below table for models M1, M2 and M3.\n",
    "\n",
    "Actual predictions\tM1\tM2\tM3\t Output\n",
    "1\t1\t0\t0\t0\n",
    "1\t1\t1\t1\t1\n",
    "1\t1\t0\t0\t0\n",
    "1\t0\t1\t0\t0\n",
    "1\t0\t1\t1\t1\n",
    "1\t0\t0\t1\t0\n",
    "1\t1\t1\t1\t1\n",
    "1\t1\t1\t1\t1\n",
    "1\t1\t1\t1\t1\n",
    "1\t1\t1\t1\t1\n",
    "\n",
    "\n",
    "18) Suppose you are building random forest model, which split a node on the attribute, that has highest information gain. In the below image, select the attribute which has the highest information gain?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A) Outlook\n",
    "B) Humidity\n",
    "C) Windy\n",
    "D) Temperature\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Information gain increases with the average purity of subsets. So option A would be the right answer.\n",
    "\n",
    "\n",
    "\n",
    "19) Which of the following is true about the Gradient Boosting trees?\n",
    "\n",
    "In each stage, introduce a new regression tree to compensate the shortcomings of existing model\n",
    "We can use gradient decent method for minimize the loss function\n",
    "A) 1\n",
    "B) 2\n",
    "C) 1 and 2\n",
    "D) None of these\n",
    "\n",
    "Solution: C\n",
    "\n",
    "Both are true and self explanatory\n",
    "\n",
    "\n",
    "\n",
    "20) True-False: The bagging is suitable for high variance low bias models?\n",
    "\n",
    "A) TRUE\n",
    "B) FALSE\n",
    "\n",
    "Solution: A\n",
    "\n",
    "The bagging is suitable for high variance low bias models or you can say for complex models.\n",
    "\n",
    "\n",
    "21) Which of the following is true when you choose fraction of observations for building the base learners in tree based algorithm?\n",
    "\n",
    "A) Decrease the fraction of samples to build a base learners will result in decrease in variance\n",
    "B) Decrease the fraction of samples to build a base learners will result in increase in variance\n",
    "C) Increase the fraction of samples to build a base learners will result in decrease in variance\n",
    "D) Increase the fraction of samples to build a base learners will result in Increase in variance\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Answer is self explanatory\n",
    "\n",
    "\n",
    "\n",
    "Context 22-23\n",
    "\n",
    "Suppose, you are building a Gradient Boosting model on data, which has millions of observations and 1000’s of features. Before building the model you want to consider the difference parameter setting for time measurement.\n",
    "\n",
    "\n",
    "22) Consider the hyperparameter “number of trees” and arrange the options in terms of time taken by each hyperparameter for building the Gradient Boosting model?\n",
    "\n",
    "Note: remaining hyperparameters are same\n",
    "\n",
    "Number of trees = 100\n",
    "Number of trees = 500\n",
    "Number of trees = 1000\n",
    "A) 1~2~3\n",
    "B) 1<2<3\n",
    "\n",
    "C) 1>2>3\n",
    "D) None of these\n",
    "\n",
    "Solution: B\n",
    "\n",
    "The time taken by building 1000 trees is maximum and time taken by building the 100 trees is minimum which is given in solution B\n",
    "\n",
    "\n",
    "\n",
    "23) Now, Consider the learning rate hyperparameter and arrange the options in terms of time taken by each hyperparameter for building the Gradient boosting model?\n",
    "\n",
    "Note: Remaining hyperparameters are same\n",
    "\n",
    "1. learning rate = 1\n",
    "2. learning rate = 2\n",
    "3. learning rate = 3\n",
    "\n",
    "A) 1~2~3\n",
    "B) 1<2<3\n",
    "\n",
    "C) 1>2>3\n",
    "D) None of these\n",
    "\n",
    "Solution: A\n",
    "\n",
    "Since learning rate doesn’t affect time so all learning rates would take equal time.\n",
    "\n",
    "\n",
    "\n",
    "24) In greadient boosting it is important use learning rate to get optimum output. Which of the following is true abut choosing the learning rate?\n",
    "\n",
    "A) Lea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}